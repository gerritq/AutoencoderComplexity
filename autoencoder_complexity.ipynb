{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Autoencoder's Compression to Measure the Complexity of Text\n",
    "\n",
    "I developed this idea while working on a project that measures the quality of EU legislation. The core concept of this notebook is to exploit the compression quality of an autoencoder as an indication of the complexity of a sentence. This idea aims to contribute to the NLP literature by measuring complexity in text.\n",
    "\n",
    "I propose to use an autoencoder model that consists of two parts: an encoder and decoder. The encoder encodes an input into a lower embedding space than the original input size through several non-linear hidden layers. This is the concept of a bottleneck, which has the lowest layer size. From this bottleneck, the decoder model has to reproduce the original input with just the information available at the bottleneck. The compression takes place exactly at this bottleneck. The replicated sentence is then compared to the original input, ensuring that the input equals the output. A simple autoencoder architecture is illustrated below (from Google Images).\n",
    "\n",
    "The idea, to reiterate, is that more complex sentences are harder to compress, which is associated with a higher loss when passing this sentence during inference time through the model. I will first load a sample sentence from a Wikipedia dataset, then build and train the model, and finally illustrate with mock sentences that the above idea indeed works.\n",
    "\n",
    "I plan to extend the method by adding further enhancements to the model (dropout layers, layer normalization, learning rate decay, etc.) to optimize the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "# For\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# For efficien computing\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Here I used a Wikipedia dataset. I will use the EU legislation set next to see the methods application to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "with open('/Users/gerritquaremba/Library/CloudStorage/GoogleDrive-g.quaremba@gmail.com/My Drive/08_NN_Kaparthy/data/wikisent2.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imanol sarriegi isasa (born 27 april 1995) is a spanish footballer who plays as a central midfielder.',\n",
       " 'indeed, the greater part of this chisian daniel cannot be said to deserve the name of a translation at all.',\n",
       " 'after moving to los angeles to become an actor, rambo started working in the real estate business.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into a list of setnences\n",
    "text = text.split('\\n')\n",
    "\n",
    "# Shuffle and select x sentences\n",
    "random.seed(42)\n",
    "random.shuffle(text)\n",
    "text=text[:1000]\n",
    "\n",
    "# DO: shuffle at this stage \n",
    "\n",
    "# Keep only sentences of a certain size and lower each\n",
    "text = [t.lower() for t in text if len(t)<=400]\n",
    "text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of sentences for training\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we are on char-level, can also change this of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of unique chars\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Length of chars\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Int to chat\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Char to int\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Longest sentence\n",
    "maxlen = len(max(text, key=len))\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "      text[i] += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imanol sarriegi isasa (born 27 april 1995) is a spanish footballer who plays as a central midfielder.                                                                                                                                                                                                                                                                                           ',\n",
       " 'indeed, the greater part of this chisian daniel cannot be said to deserve the name of a translation at all.                                                                                                                                                                                                                                                                                     ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check sentences\n",
    "text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input and output tensors, which are the same for the autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get in and out\n",
    "inout = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    inout.append([char2int[character] for character in text[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 384])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "inout = torch.tensor(inout, dtype=torch.long)\n",
    "inout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inout` is of size `(B, T)`. We have set `T` above to be a bit shorter. A batch is an epoch here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Architecture\n",
    "\n",
    "Below implements the AC. The optimal architecture is to be determined. Currently, the bottleneck is of dim `(8,8)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE\n",
    "class AE(nn.Module): # inherent from the nn class\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create embedding matrix\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # This is a 65 x n_embed matrix; weights are inintialised at random\n",
    "        \n",
    "    \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_embed, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_embed)\n",
    "        )\n",
    "   \n",
    "    def forward(self, idx):\n",
    "\n",
    "       # Get embeddings\n",
    "        embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # Encode\n",
    "        encoded = self.encoder(embeddings)\n",
    "\n",
    "        # Decode\n",
    "        reconstructed = self.decoder(encoded)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(reconstructed, embeddings)\n",
    "\n",
    "        return reconstructed, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model and HPs\n",
    "# Hyperparameters\n",
    "n_embed=64\n",
    "batch_size=len(text)\n",
    "max_iters = 500\n",
    "learning_rate=1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model\n",
    "model = AE()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step[0/500] loss: 1.1515729427337646\n",
      "Step[100/500] loss: 0.18843428790569305\n",
      "Step[200/500] loss: 0.09664537757635117\n",
      "Step[300/500] loss: 0.050881076604127884\n",
      "Step[400/500] loss: 0.02575305663049221\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# PyTorch optimiser\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iters):\n",
    "\n",
    "    # Forward pass\n",
    "    _, loss = model(inout)\n",
    "\n",
    "    # Set gradients to 0\n",
    "    optimizer.zero_grad(set_to_none=True) # more efficient\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # Adam, update gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print final loss (batch loss)\n",
    "    if step % 100  == 0:\n",
    "        print(f\"Step[{step}/{max_iters}] loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the training loss we cann tell that the optimisation works. But there is considerable room for improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression as a measure for text complexity?\n",
    "\n",
    "The intuition is that a sentence that is more complex (longer, more complex syntax, etc) is more difficult to compress and and rebuild, whicch is indicated with a higher loss during inference. Below I test exaclty this. For the evaluation, I took a simple comparsion of the loss. But a more intuitive measure would be to take the logits and compute the probability of the sentence. (Is this entropy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get loss for a sentence\n",
    "def compression_score(model, sent):\n",
    "\n",
    "\n",
    "    model.eval() # eval mode\n",
    "  \n",
    "    # Pre sentence for input into the mdoel\n",
    "    sent = sent.lower()\n",
    "\n",
    "    # Apply padding\n",
    "    while len(sent)<maxlen:\n",
    "      sent += ' '\n",
    "\n",
    "    # Sentecne to idx, and tensor with batch dim 1\n",
    "    sent_idx = [char2int[character] for character in sent] # \n",
    "    sent_idx = torch.tensor(sent_idx, dtype=torch.long).view(1, maxlen)\n",
    "    \n",
    "    # Output the logits\n",
    "    _, loss = model(sent_idx)\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison of scores\n",
    "compression_score(model, \"Hello there\") < compression_score(model, \"Hello there this is likely more difficult, becasue it contains more text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses differ considerably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009428826160728931"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difficult sentence\n",
    "compression_score(model, \"Hello there. This is likely more difficult, because it contains more text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009569653775542974"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Easier sentence\n",
    "compression_score(model, \"Hello there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One concern that may drive the above result is that sheer length is driving the comeplexity of a sentence. Let us test same length but more complicated syntax/grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same length, different complexity\n",
    "compression_score(model, \"This is a sentence that has the same length as the other sentence.\") < compression_score(model, \"This is, by far, a more convoluted sentence; hence more complex.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004809746518731117"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Easy sentence\n",
    "compression_score(model, \"This is a sentence that has the same length as the other sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008957922458648682"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complex sentence\n",
    "compression_score(model, \"This is, by far, a more convoluted sentence; hence more complex.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to look good! More in the near future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
